{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "2018-05-15 17:43:46,337 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, explained_variance_score, mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten, Activation\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>The good: mini doughnuts and americano friendl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>I really like the décor of the place and the w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Great service, clean and great food, this plac...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>This place has never failed on me! \\nLate nigh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>This is a fast food spin off from Paramount Fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text  useful  funny\n",
       "0      3  The good: mini doughnuts and americano friendl...       1      0\n",
       "1      3  I really like the décor of the place and the w...       0      0\n",
       "2      5  Great service, clean and great food, this plac...       1      0\n",
       "3      5  This place has never failed on me! \\nLate nigh...       0      0\n",
       "4      2  This is a fast food spin off from Paramount Fi...       1      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/nikita/PycharmProjects/natural_language/data/test_prepocess.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "train_batch = train_data[0:100000]\n",
    "test_batch = test_data[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentences_vector(sentence):\n",
    "    words = sentence.split()\n",
    "    result = np.zeros(100)\n",
    "    size = 0\n",
    "    for word in words:\n",
    "        if word in embeding_model.wv.vocab:\n",
    "            result+=embeding_model.wv[word]\n",
    "            size+=1\n",
    "    if size == 0:\n",
    "        size = 1\n",
    "    result /= size\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions, target, title=\"Confusion matrix\"):\n",
    "    print('cross_val_score              %s' % accuracy_score(target, predictions))\n",
    "#     print('explained_variance_score     %s' % explained_variance_score(target, predictions))\n",
    "#     print('mean_absolute_error          %s' % mean_absolute_error(target, predictions))\n",
    "#     print('mean_squared_error           %s' % mean_squared_error(target, predictions))\n",
    "#     print('mean_squared_log_error       %s' % mean_squared_log_error(target, predictions))\n",
    "#     print('r2_score                     %s' % r2_score(target, predictions))\n",
    "    print()\n",
    "    cm = confusion_matrix(target, predictions)\n",
    "    print('confusion matrix\\n %s' % cm)\n",
    "    print('(row=expected, col=predicted)')\n",
    "    \n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_normalized, title + ' Normalized')\n",
    "    \n",
    "def plot_confusion_matrix(cm, title='Матрица ошибок', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    my_tags = pd.unique(data.stars)\n",
    "    tick_marks = np.arange(len(my_tags))\n",
    "    target_names = my_tags\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Исходные метки')\n",
    "    plt.xlabel('Полученные метки')\n",
    "\n",
    "def predict(vectorizer, classifier, data):\n",
    "    data_features = vectorizer.transform(data.text)\n",
    "    predictions = classifier.predict(data_features)\n",
    "    target = data.stars\n",
    "    evaluate_prediction(predictions, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-15 17:44:19,317 : INFO : loading Word2Vec object from Yelp_embedings\n",
      "2018-05-15 17:44:22,990 : INFO : loading wv recursively from Yelp_embedings.wv.* with mmap=None\n",
      "2018-05-15 17:44:22,991 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-05-15 17:44:22,992 : INFO : loading vocabulary recursively from Yelp_embedings.vocabulary.* with mmap=None\n",
      "2018-05-15 17:44:22,993 : INFO : loading trainables recursively from Yelp_embedings.trainables.* with mmap=None\n",
      "2018-05-15 17:44:22,993 : INFO : setting ignored attribute cum_table to None\n",
      "2018-05-15 17:44:22,997 : INFO : loaded Yelp_embedings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101865"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding_model = Word2Vec.load('Yelp_embedings')\n",
    "len(embeding_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 477 ms, total: 1min 32s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_features = []\n",
    "for sentense in train_batch.text:\n",
    "    train_data_features.append(build_sentences_vector(sentense))\n",
    "test_data_transform = []\n",
    "for sentense in test_batch.text:\n",
    "    test_data_transform.append(build_sentences_vector(sentense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = pd.DataFrame(train_data_features)\n",
    "test_data_transform = pd.DataFrame(test_data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 189 ms, sys: 200 ms, total: 389 ms\n",
      "Wall time: 599 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "big_X = train_data_features.append(test_data_transform)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(big_X)\n",
    "train_data_features = scaler.transform(train_data_features)\n",
    "test_data_transform = scaler.transform(test_data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47768725, 0.50261539, 0.7106637 , 0.60011637, 0.30857185,\n",
       "       0.46072202, 0.58532382, 0.35489181, 0.56592655, 0.52509615,\n",
       "       0.34207966, 0.46469938, 0.28336177, 0.48729129, 0.56763504,\n",
       "       0.48545882, 0.3633329 , 0.50221415, 0.4790287 , 0.44466971,\n",
       "       0.60395302, 0.21026502, 0.71012634, 0.61785903, 0.32811395,\n",
       "       0.40180564, 0.42955268, 0.52559043, 0.41086256, 0.41418741,\n",
       "       0.7674902 , 0.54301586, 0.61783318, 0.58074126, 0.55888046,\n",
       "       0.54453445, 0.36168717, 0.54021471, 0.45886499, 0.64497301,\n",
       "       0.68964035, 0.66150094, 0.42380001, 0.29542307, 0.2645298 ,\n",
       "       0.64970587, 0.59399217, 0.70192113, 0.67110966, 0.66881021,\n",
       "       0.52171434, 0.47996132, 0.52243193, 0.59537827, 0.46571829,\n",
       "       0.55339619, 0.73363276, 0.42394804, 0.23819312, 0.43204729,\n",
       "       0.68764806, 0.66526823, 0.39650339, 0.48692192, 0.21172303,\n",
       "       0.69001408, 0.35145527, 0.39906459, 0.34272367, 0.48391983,\n",
       "       0.44863189, 0.6544023 , 0.37294765, 0.48754314, 0.75848183,\n",
       "       0.49511291, 0.5709513 , 0.5161289 , 0.75845332, 0.39976307,\n",
       "       0.49585772, 0.5884334 , 0.4852264 , 0.49504398, 0.32936473,\n",
       "       0.46630647, 0.48406719, 0.66319702, 0.62884117, 0.29202446,\n",
       "       0.6356838 , 0.60575013, 0.64485996, 0.40819167, 0.50405794,\n",
       "       0.52419061, 0.69306765, 0.64954838, 0.29577478, 0.49781375])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 107)               10807     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10800     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 22,213\n",
      "Trainable params: 22,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(107, input_dim=100, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "epochs = 30\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y = np_utils.to_categorical(train_batch.stars)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (6,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features[0].shape, Y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85000 samples, validate on 15000 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 0.1164 - acc: 0.4320 - val_loss: 0.1112 - val_acc: 0.4285\n",
      "Epoch 2/30\n",
      " - 5s - loss: 0.1060 - acc: 0.4700 - val_loss: 0.1022 - val_acc: 0.5087\n",
      "Epoch 3/30\n",
      " - 5s - loss: 0.0995 - acc: 0.5293 - val_loss: 0.0975 - val_acc: 0.5306\n",
      "Epoch 4/30\n",
      " - 5s - loss: 0.0958 - acc: 0.5467 - val_loss: 0.0943 - val_acc: 0.5563\n",
      "Epoch 5/30\n",
      " - 5s - loss: 0.0936 - acc: 0.5546 - val_loss: 0.0927 - val_acc: 0.5551\n",
      "Epoch 6/30\n",
      " - 5s - loss: 0.0920 - acc: 0.5610 - val_loss: 0.0911 - val_acc: 0.5679\n",
      "Epoch 7/30\n",
      " - 5s - loss: 0.0909 - acc: 0.5669 - val_loss: 0.0903 - val_acc: 0.5699\n",
      "Epoch 8/30\n",
      " - 5s - loss: 0.0901 - acc: 0.5718 - val_loss: 0.0897 - val_acc: 0.5799\n",
      "Epoch 9/30\n",
      " - 5s - loss: 0.0894 - acc: 0.5750 - val_loss: 0.0891 - val_acc: 0.5764\n",
      "Epoch 10/30\n",
      " - 5s - loss: 0.0889 - acc: 0.5771 - val_loss: 0.0885 - val_acc: 0.5816\n",
      "Epoch 11/30\n",
      " - 5s - loss: 0.0885 - acc: 0.5798 - val_loss: 0.0882 - val_acc: 0.5840\n",
      "Epoch 12/30\n",
      " - 5s - loss: 0.0880 - acc: 0.5825 - val_loss: 0.0878 - val_acc: 0.5824\n",
      "Epoch 13/30\n",
      " - 5s - loss: 0.0877 - acc: 0.5847 - val_loss: 0.0875 - val_acc: 0.5846\n",
      "Epoch 14/30\n",
      " - 5s - loss: 0.0874 - acc: 0.5862 - val_loss: 0.0872 - val_acc: 0.5864\n",
      "Epoch 15/30\n",
      " - 5s - loss: 0.0872 - acc: 0.5873 - val_loss: 0.0870 - val_acc: 0.5890\n",
      "Epoch 16/30\n",
      " - 5s - loss: 0.0869 - acc: 0.5887 - val_loss: 0.0868 - val_acc: 0.5887\n",
      "Epoch 17/30\n",
      " - 5s - loss: 0.0867 - acc: 0.5904 - val_loss: 0.0870 - val_acc: 0.5866\n",
      "Epoch 18/30\n",
      " - 5s - loss: 0.0865 - acc: 0.5915 - val_loss: 0.0866 - val_acc: 0.5899\n",
      "Epoch 19/30\n",
      " - 5s - loss: 0.0863 - acc: 0.5923 - val_loss: 0.0863 - val_acc: 0.5923\n",
      "Epoch 20/30\n",
      " - 5s - loss: 0.0862 - acc: 0.5929 - val_loss: 0.0861 - val_acc: 0.5929\n",
      "Epoch 21/30\n",
      " - 5s - loss: 0.0860 - acc: 0.5942 - val_loss: 0.0860 - val_acc: 0.5939\n",
      "Epoch 22/30\n",
      " - 5s - loss: 0.0859 - acc: 0.5940 - val_loss: 0.0860 - val_acc: 0.5920\n",
      "Epoch 23/30\n",
      " - 5s - loss: 0.0857 - acc: 0.5953 - val_loss: 0.0857 - val_acc: 0.5953\n",
      "Epoch 24/30\n",
      " - 5s - loss: 0.0856 - acc: 0.5970 - val_loss: 0.0856 - val_acc: 0.5949\n",
      "Epoch 25/30\n",
      " - 5s - loss: 0.0855 - acc: 0.5964 - val_loss: 0.0857 - val_acc: 0.5937\n",
      "Epoch 26/30\n",
      " - 5s - loss: 0.0854 - acc: 0.5966 - val_loss: 0.0855 - val_acc: 0.5962\n",
      "Epoch 27/30\n",
      " - 5s - loss: 0.0853 - acc: 0.5973 - val_loss: 0.0854 - val_acc: 0.5958\n",
      "Epoch 28/30\n",
      " - 5s - loss: 0.0852 - acc: 0.5981 - val_loss: 0.0855 - val_acc: 0.5971\n",
      "Epoch 29/30\n",
      " - 5s - loss: 0.0851 - acc: 0.5989 - val_loss: 0.0852 - val_acc: 0.5955\n",
      "Epoch 30/30\n",
      " - 5s - loss: 0.0850 - acc: 0.5989 - val_loss: 0.0851 - val_acc: 0.5960\n",
      "CPU times: user 3min 31s, sys: 8.69 s, total: 3min 40s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb72fec36a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(np.array(train_data_features), np.array(Y), verbose=2, epochs=30, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 51 ms, total: 1.32 s\n",
      "Wall time: 997 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = model.predict(test_data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15347077,  3.1922393 ,  3.5636344 ,  5.342968  , 22.865559  ,\n",
       "       64.882126  ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 12us/step\n",
      "CPU times: user 811 ms, sys: 52.3 ms, total: 863 ms\n",
      "Wall time: 611 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_stars = np_utils.to_categorical(test_batch.stars)\n",
    "score = model.evaluate(test_data_transform, test_stars, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.85400000190735"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
